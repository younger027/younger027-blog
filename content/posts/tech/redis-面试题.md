---
title: "Redis 面试题"
date: 2023-07-25T17:53:00+08:00
draft: true
---

1. 持久化怎么实现的
  aof：append only file。持续写文件到buffer ring中，然后根据参数完成fsync操作。aof文件会有重写操作，节省看见，以及宕机恢复操作的时间。

  rdb：内存快照。对某一时刻的redis的数据情况进行快照存储。


2. zset怎么做延迟队列

zadd key Val score：job数据存储在Val，score代表任务执行时间。

zrangebyscore time，拿当前的时间戳做比较。取可执行的任务。

zremrange 移除这些任务。

3. 哪些操作会阻塞redis

   Redis单线程处理IO请求性能瓶颈主要包括2个方面：任意一个请求在server中一旦发生耗时，都会影响整个server的性能 也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。

   耗时的操作包括：

   - **操作bigkey**：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时
   - **使用复杂度过高的命令**：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据
   - **大量key集中过期**：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长
   - **淘汰策略**：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会 造成耗时变长。
   - **AOF刷盘开启always机制**：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能
   - **主从全量同步生成RDB**：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久

- Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能。当然，只针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的


4. redis分布式锁用过吗？说下咋用的，哪些场景需要用

   ```c
   SET lock_key unique_Val NX PX 10000 
   ```

   - lock_key 就是 key 键；
   - unique_Val 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；
   - NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；
   - PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。

   解锁时因为要判断是否是加锁者来解锁，需要完成判断和解锁两个操作。故借助lua脚本完成。

   难点：

   1. 超时时间不好设置。设置短了，任务还未执行完成，其他的线程就可以拿到锁。设置长了，影响性能。为了解决这个问题可以加一个看门狗机制。也就是新增一个守护线程专门监控这种情况。当任务未执行完成，锁快要过期时，续约锁时间。当任务执行完成，但锁还未过期，直接销毁锁即可
   2. 主从模式的时候，redis异步复制数据，这将导致分布式锁的不可靠性。如果在主节点获取锁后，其他节点还未同步到锁。主节点宕机了。那么新的主节点还是可以获取到锁。那么如何解决呢。就是redis的redlock红锁

   

   ##### Redis的redlock

   为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。

   它是基于**多个 Redis 节点**的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点。

   Redlock 算法的基本思路，**是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败**。

   这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。

   Redlock 算法加锁三个过程：

   - 第一步是，客户端获取当前时间（t1）。
   - 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作：
     - 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。
     - 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。
   - 第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 < 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。

   可以看到，加锁成功要同时满足两个条件（*简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功*）：

   - 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁；
   - 条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间。

   加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁最初设置的过期时间」减去「客户端从大多数节点获取锁的总耗时（t2-t1）」。如果计算的结果已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。

   加锁失败后，客户端向**所有 Redis 节点发起释放锁的操作**，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。

   

5. 缓存穿透、雪崩的解决方案

   缓存穿透：同一时刻大量请求访问不存在的数据。

   - 缓存空值：可以在缓存中缓存空值来防止压力给到mysql。但是这也不是万能的。缓存空值会浪费我们的空间，如果缓存的空值比较多。还会影响redis的命中率。淘汰了我们正常的热点数据。所以生产环境，我们需要监控一些null空值的数量。避免浪费空间和对正常热数据的影响。我们还可以使用布隆过滤器来处理。
   - 布隆过滤器：查询布隆过滤器，判断数据是否存在。
     - 用法：存储数据时，布隆过滤器会先以hash函数来判断数据的落位bit，以bit位0,1来标示数据是否存在。读取数据时，就可以根据0，1判断是否数据是否存在，而不用去缓存和数据库查找数据。
     - 缺点：1.有hash函数就有hash碰撞，所以就有可能将并不在集合中的元素判断在里面。2.不支持删除元素
     - 解决方案：针对第一点，可以采用多个hash函数来确定bit位，减少误差。多个hash都是1的时候，数据真正存在。(存在疑问？多个hash函数不是更加加大了碰撞的几率嘛。确实是这样的。但是前期数据量比较少的时候，多个hash函数可以显著增加误判率，但是随着越来越多的“1”被占用。碰撞的几率也会越来越大。所以需要去关注布隆过滤器的负载量)
     - 另外布隆过滤器有误判的可能。布隆过滤器判断存在数据存在时，不一定存在(因为hash碰撞占位)。数据不存在时，肯定是不存在，这个是准确的。

   

   缓存雪崩：有大量的key在同一时间过期，或者redis故障

   - 分散key的失效时间。设置过期时间时，可以加一个时间偏移量
   - 设置key不过期。这个方式比较暴力。但是key并不是真正的会一直存在。redis的多种内存淘汰机制可能会清理掉数据。因此我们需要其他的机制来监控缓存的数据。
     1. 可以将缓存更新的任务交由后台线程完成。定时扫描key是否存在。是否需要更新最新数据。业务不负责缓存数据的更新。但是定时扫描的定时需要trade-off，均衡业务的容忍度。
     2. 业务请求时发现数据不存在，往mq中写一条消息。交由消费者完成数据的更新和加载。当然执行前可以检测下，数据是否已经存在。
   - 互斥锁。保证不会有大量的请求打到mysql。只有一个请求完成cache aside方式的数据回写。

   

   缓存击穿：热点key数据过期

   - 互斥锁
   - 任务key不过期

   

1. 如何保证缓存-db一致性

   - cache aside方式。选择先更新db，再删除缓存。另外可加消费mysql binlog 的方式。保证更新db和删除操作都执行成功。

2. redis怎么做限流

   可以依赖redis单线程和其支持的数据结构完成限流操作。例如：依赖list结构实现令牌桶，固定时间生成token，当令牌token为空时，新来的请求阻塞或者直接返回失败。下面是go-zero框架luaa脚本实现令牌桶的方式：

   ```lua
   script = `local rate = tonumber(ARGV[1])
   local capacity = tonumber(ARGV[2])
   local now = tonumber(ARGV[3])
   local requested = tonumber(ARGV[4])
   
   //过期到下次填充前的时间
   local fill_time = capacity/rate
   local ttl = math.floor(fill_time*2)
   
   local last_tokens = tonumber(redis.call("get", KEYS[1]))
   if last_tokens == nil then
       last_tokens = capacity
   end
   
   local last_refreshed = tonumber(redis.call("get", KEYS[2]))
   if last_refreshed == nil then
       last_refreshed = 0
   end
   
   //计算上次更新token的时间差，计算出需要补多少token。判断是否满足请求需要的request
   //计算剩余的token量
   local delta = math.max(0, now-last_refreshed)
   local filled_tokens = math.min(capacity, last_tokens+(delta*rate))
   local allowed = filled_tokens >= requested
   local new_tokens = filled_tokens
   if allowed then
       new_tokens = filled_tokens - requested
   end
   
   redis.call("setex", KEYS[1], ttl, new_tokens)
   redis.call("setex", KEYS[2], ttl, now)
   
   return allowed`
   ```

   

8. redis中server和client通信方式？

   - 简单文本通信协议

9. redis淘汰策略

10. - noeviction（不淘汰策略）：当内存不足以容纳新写入数据时，新写入操作会报错"OOM（Out of Memory）"。默认情况下，Redis就采用了此种淘汰策略。
    - allkeys-lru（最近最少使用淘汰策略）：Redis会根据键的最近最少使用时间来淘汰数据。当内存不足以容纳新写入数据时，从所有键中选择最近最少使用的数据淘汰。
    - volatile-lru（设置过期时间的最近最少使用淘汰策略）：Redis会根据键的最近最少使用时间来淘汰键值对，但只会针对设定了过期时间的键值对进行淘汰。
    - volatile-ttl（设置过期时间淘汰策略）：Redis会根据键值对的剩余存活时间来淘汰数据。当内存不足以容纳新写入数据时，优先淘汰存活时间较短的数据。此种淘汰策略适用于只关心最近活跃数据的场景。


11. 缓存预热？如何做

缓存预热：redis启动前缓存热门数据

nginx+lua将流量请求打到kafka中，让kafka抗高并发的量。然后数据写入storm中，storm完成M份Top N数据的生成。然后聚合生成热门数据存到zk中，新起任务完成读取数据，写入redis。完成预热缓存。

12. Redis 应用题: 设计一个每日日活用户统计功能
需求: 我们的网站每天用户访问之后，UserID 将会被收集到我们的 Redis 数据存储（需要你思考如何存储），我们的后台需要支持一个功能:
1.API - 支持查询 某一天访问的用户 & 前一天并没有访问我们的网站
Request : {"date": "20230216"}
Response: {"user_ids": [1,3,4]}
2.API 拓展 - 在支持(1) 的基础上，支持根据指定批量日期范围查询这个数据
Request : {"dates": ["20230216", "20230217"]}
Response: {"list": [{"date": "20230216", "user_ids": [1,3,4]}, {"date": "20230216", "user_ids": [2,9]}]}
数据量级:
Level0: 每日访问用户数在 1K 以内
Level1: 每日访问用户数在 10K 以内
考察点1: redis 数据存储如何设计，应该使用什么数据结构，怎么记录和实现功能

答案：
1.按照天的维度将用户放到set结构中(key是日期，value是UserID)，针对查询需求，可以将两天的set做diff操作，生成到一个new set中。
2.如果要支持指定批量日期范围查询，首先需要明确最早日期，日期数量是否有限制？qps是多少(每条执行多少请求和计算量有关)，如果不高的话，可以实时diff计算，
一般情况下，latency会比较长。
latency过长的解决方案：
1.缓存：new set已经放入到了redis中，如果请求中的日期过多，影响了latency的性能，那么可以使用进程内缓存，此时就要考虑缓存的大小如何设计？(设计的标准应该是满足当前qps，多少数据从redis拿，多少从进程缓存拿，可以满足qps需求)，考虑设计缓存淘汰方式(LRU、LFU or 其他？，这里选择LFU，淘汰使用频率最少的比较合理)。
2.升级机器配置，升级cpu配置，升级redis配置。使得机器cpu不是瓶颈(内存应该不是瓶颈，不需要考虑)，升级redis的cpu配置，主从或者分片。

数量级问题需要注意的点：1k以内的set，userid是int类型的话8byte，一个日期的key存储占8k(set是hash结构和intset，那就按照正常的数组大小评估，就是8k)，不算是大key(大key的标准和结构中的数量和元素的大小相关)，那么diff完成的也不是大key。这个时候就还是考虑latency的问题了，按照上面方案解决就行。

如果真的数量级到了大key，那么一般方案也就是拆分成多个key，此时内存友好，但是cpu就不友好了。解决的话，可以升级配置。或者redis集群部署，分散压力。